{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "duckietown_torch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPeyo989GoLtdpsI2pgsxMd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aeapolimi/duckietown/blob/master/duckietown_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXCi29akvoPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b108358-ad11-41bb-a3a2-f57772982a07"
      },
      "source": [
        "import os \n",
        "if not os.path.isdir('gym-duckietown') and not os.path.isdir('../gym-duckietown'):\n",
        "  branch = \"master\" #@param ['master', 'daffy']\n",
        "  !git clone --branch {branch} https://github.com/duckietown/gym-duckietown.git\n",
        "  !pip3 install -e gym-duckietown\n",
        "if \"/gym-duckietown\" not in os.getcwd():\n",
        "  os.chdir('gym-duckietown')\n",
        "!apt install xvfb -y\n",
        "!pip3 install pyvirtualdisplay\n",
        "from pyvirtualdisplay import Display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "%matplotlib inline"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBv2f7yhwpl4"
      },
      "source": [
        "import ast\n",
        "import argparse\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Duckietown Specific\n",
        "from learning.reinforcement.pytorch.ddpg import DDPG\n",
        "from learning.utils.env import launch_env\n",
        "from learning.reinforcement.pytorch.utils import seed, evaluate_policy, ReplayBuffer\n",
        "from learning.utils.wrappers import NormalizeWrapper, ImgWrapper, \\\n",
        "    DtRewardWrapper, ActionWrapper, ResizeWrapper\n",
        "\n",
        "import gym\n",
        "import gym_duckietown"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tng7iUF81fx9"
      },
      "source": [
        "map_name = \"Duckietown-small_loop-v0\" #@param ['Duckietown-straight_road-v0','Duckietown-4way-v0','Duckietown-udem1-v0','Duckietown-small_loop-v0','Duckietown-small_loop_cw-v0','Duckietown-zigzag_dists-v0','Duckietown-loop_obstacles-v0','Duckietown-loop_pedestrians-v0']"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb4A1a7YykX2"
      },
      "source": [
        "def _train(seeds, eval_freq, max_timesteps, save_models, expl_noise,\n",
        "           batch_size, discount, tau, policy_noise, noise_clip, policy_freq,\n",
        "           env_timesteps, replay_buffer_max_size, model_dir):   \n",
        "    if not os.path.exists(\"./results\"):\n",
        "        os.makedirs(\"./results\")\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "        \n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()\n",
        "    env = gym.make(map_name, accept_start_angle_deg=4)\n",
        "    print(\"Initialized environment\")\n",
        "\n",
        "    # Wrappers\n",
        "    env = ResizeWrapper(env)\n",
        "    env = NormalizeWrapper(env)\n",
        "    env = ImgWrapper(env) # to make the images from 160x120x3 into 3x160x120\n",
        "    env = ActionWrapper(env)\n",
        "    env = DtRewardWrapper(env)\n",
        "    print(\"Initialized Wrappers\")\n",
        "    \n",
        "    # Set seeds\n",
        "    seed(seeds)\n",
        "\n",
        "    state_dim = env.observation_space.shape\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    # Initialize policy\n",
        "    policy = DDPG(state_dim, action_dim, max_action, net_type=\"cnn\")\n",
        "    replay_buffer = ReplayBuffer(replay_buffer_max_size)\n",
        "    print(\"Initialized DDPG\")\n",
        "    \n",
        "    # Evaluate untrained policy\n",
        "    evaluations= [evaluate_policy(env, policy)]\n",
        "   \n",
        "    total_timesteps = 0\n",
        "    timesteps_since_eval = 0\n",
        "    episode_num = 0\n",
        "    done = True\n",
        "    episode_reward = None\n",
        "    env_counter = 0\n",
        "    reward = 0\n",
        "    episode_timesteps = 0\n",
        "    \n",
        "    print(\"Starting training\")\n",
        "    while total_timesteps < max_timesteps:\n",
        "        \n",
        "        print(\"timestep: {} | reward: {}\".format(total_timesteps, reward))\n",
        "            \n",
        "        if done:\n",
        "            if total_timesteps != 0:\n",
        "                print((\"Total T: %d Episode Num: %d Episode T: %d Reward: %f\") % (\n",
        "                    total_timesteps, episode_num, episode_timesteps, episode_reward))\n",
        "                policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau)\n",
        "\n",
        "                # Evaluate episode\n",
        "                if timesteps_since_eval >= eval_freq:\n",
        "                    timesteps_since_eval %= eval_freq\n",
        "                    evaluations.append(evaluate_policy(env, policy))\n",
        "                    print(\"rewards at time {}: {}\".format(total_timesteps, evaluations[-1]))\n",
        "\n",
        "                    if save_models:\n",
        "                        policy.save(filename='ddpg', directory=model_dir)\n",
        "                    np.savez(\"./results/rewards.npz\",evaluations)\n",
        "\n",
        "            # Reset environment\n",
        "            env_counter += 1\n",
        "            obs = env.reset()\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            episode_timesteps = 0\n",
        "            episode_num += 1\n",
        "\n",
        "        # Select action randomly or according to policy\n",
        "        if total_timesteps < start_timesteps:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = policy.predict(np.array(obs))\n",
        "            if expl_noise != 0:\n",
        "                action = (action + np.random.normal(\n",
        "                    0,\n",
        "                    expl_noise,\n",
        "                    size=env.action_space.shape[0])\n",
        "                          ).clip(env.action_space.low, env.action_space.high)\n",
        "\n",
        "        # Perform action\n",
        "        new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "        if episode_timesteps >= env_timesteps:\n",
        "            done = True\n",
        "\n",
        "        done_bool = 0 if episode_timesteps + 1 == env_timesteps else float(done)\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Store data in replay buffer\n",
        "        replay_buffer.add(obs, new_obs, action, reward, done_bool)\n",
        "\n",
        "        obs = new_obs\n",
        "\n",
        "        episode_timesteps += 1\n",
        "        total_timesteps += 1\n",
        "        timesteps_since_eval += 1\n",
        "    \n",
        "    print(\"Training done, about to save..\")\n",
        "    policy.save(filename='ddpg', directory=model_dir)\n",
        "    print(\"Finished saving..should return now!\")\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPGPrgmUzS2t",
        "outputId": "4d6cbc41-6488-4977-b22a-422a6a6012c1"
      },
      "source": [
        "seeds = 0\n",
        "start_timesteps=1e4\n",
        "eval_freq=5e3 # How often (time steps) we evaluate\n",
        "max_timesteps=1e6  # Max time steps to run environment for\n",
        "save_models=\"store_true\"  # Whether or not models are saved\n",
        "expl_noise=0.1  # Std of Gaussian exploration noise\n",
        "batch_size=32  # Batch size for both actor and critic\n",
        "discount=0.99  # Discount factor\n",
        "tau=0.005  # Target network update rate\n",
        "policy_noise=0.2  # Noise added to target policy during critic update\n",
        "noise_clip=0.5 # Range to clip target policy noise\n",
        "policy_freq=2  # Frequency of delayed policy updates\n",
        "env_timesteps=500  # Frequency of delayed policy updates\n",
        "replay_buffer_max_size=10000  # Maximum number of steps to keep in the replay buffer\n",
        "model_dir='learning/reinforcement/pytorch/models/'\n",
        "\n",
        "_train(\n",
        "    seeds=seeds,\n",
        "    eval_freq=eval_freq,\n",
        "    max_timesteps=max_timesteps,\n",
        "    save_models=save_models,\n",
        "    expl_noise=expl_noise,\n",
        "    batch_size=batch_size,\n",
        "    discount=discount,\n",
        "    tau=tau,\n",
        "    policy_noise=policy_noise,\n",
        "    noise_clip=noise_clip,\n",
        "    policy_freq=policy_freq,\n",
        "    env_timesteps=env_timesteps,\n",
        "    replay_buffer_max_size=replay_buffer_max_size,\n",
        "    model_dir=model_dir\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initialized environment\n",
            "Initialized Wrappers\n",
            "Starting DDPG init\n",
            "Initialized Actor\n",
            "Initialized Target+Opt [Actor]\n",
            "Initialized Critic\n",
            "Initialized Target+Opt [Critic]\n",
            "Initialized DDPG\n",
            "Starting training\n",
            "timestep: 0 | reward: 0\n",
            "timestep: 1 | reward: 1.5604179499897048\n",
            "timestep: 2 | reward: 2.563289032970535\n",
            "timestep: 3 | reward: 2.314672058805505\n",
            "timestep: 4 | reward: 2.7746287910360845\n",
            "timestep: 5 | reward: 2.4312907277831823\n",
            "timestep: 6 | reward: 2.3168494020717096\n",
            "timestep: 7 | reward: 1.7397153891246746\n",
            "timestep: 8 | reward: 2.7183708989972986\n",
            "timestep: 9 | reward: 2.817079855858666\n",
            "timestep: 10 | reward: 2.226919846211483\n",
            "timestep: 11 | reward: 2.6565872055575093\n",
            "timestep: 12 | reward: 3.3593962557046515\n",
            "timestep: 13 | reward: 3.376330798627011\n",
            "timestep: 14 | reward: 2.7987914343189813\n",
            "timestep: 15 | reward: 2.848069400739399\n",
            "timestep: 16 | reward: 3.23728229087465\n",
            "timestep: 17 | reward: 3.4409822905569722\n",
            "timestep: 18 | reward: 3.6785156369097582\n",
            "timestep: 19 | reward: 10.17529010308998\n",
            "timestep: 20 | reward: 3.3205172703701487\n",
            "timestep: 21 | reward: -10\n",
            "Total T: 21 Episode Num: 1 Episode T: 21 Reward: 52.354997\n",
            "timestep: 22 | reward: 3.9357475081696505\n",
            "timestep: 23 | reward: 3.7595332051905928\n",
            "timestep: 24 | reward: 10.572042993753598\n",
            "timestep: 25 | reward: 10.538738130880933\n",
            "timestep: 26 | reward: 10.505433268008266\n",
            "timestep: 27 | reward: 10.45148640896662\n",
            "timestep: 28 | reward: 10.043942386150036\n",
            "timestep: 29 | reward: 10.049098348863254\n",
            "timestep: 30 | reward: 3.879522175646982\n",
            "timestep: 31 | reward: 10.555594686510386\n",
            "timestep: 32 | reward: 10.344763266300902\n",
            "timestep: 33 | reward: 10.721383136918705\n",
            "timestep: 34 | reward: 10.371196451513045\n",
            "timestep: 35 | reward: 3.720770227433703\n",
            "timestep: 36 | reward: 3.6585607127278554\n",
            "timestep: 37 | reward: 10.786063047328145\n",
            "timestep: 38 | reward: 10.492661174800187\n",
            "timestep: 39 | reward: 3.6436708029666987\n",
            "timestep: 40 | reward: 10.083231839110587\n",
            "timestep: 41 | reward: -10\n",
            "Total T: 41 Episode Num: 2 Episode T: 20 Reward: 148.113440\n",
            "timestep: 42 | reward: 10.99797387964841\n",
            "timestep: 43 | reward: 10.031628649711621\n",
            "timestep: 44 | reward: 10.84198437664365\n",
            "timestep: 45 | reward: 10.643261408647998\n",
            "timestep: 46 | reward: 10.18132733702157\n",
            "timestep: 47 | reward: 3.6826817809322945\n",
            "timestep: 48 | reward: 10.765608654829162\n",
            "timestep: 49 | reward: 10.09422689034266\n",
            "timestep: 50 | reward: 10.419568198761544\n",
            "timestep: 51 | reward: 10.399063261131296\n",
            "timestep: 52 | reward: 3.692239744978992\n",
            "timestep: 53 | reward: 3.9376038824471444\n",
            "timestep: 54 | reward: 3.9102685014824132\n",
            "timestep: 55 | reward: 10.132292931232731\n",
            "timestep: 56 | reward: 3.881575191982774\n",
            "timestep: 57 | reward: 10.02533752618686\n",
            "timestep: 58 | reward: 10.562312947132902\n",
            "timestep: 59 | reward: 10.558517725730486\n",
            "timestep: 60 | reward: 10.485149826978272\n",
            "timestep: 61 | reward: 10.201181083629578\n",
            "timestep: 62 | reward: 3.652765268661375\n",
            "timestep: 63 | reward: 3.7111728143237865\n",
            "timestep: 64 | reward: 3.8433591234828546\n",
            "timestep: 65 | reward: 10.322397313207198\n",
            "timestep: 66 | reward: 10.090533359562786\n",
            "timestep: 67 | reward: 10.194917375954667\n",
            "timestep: 68 | reward: 10.356566369979662\n",
            "timestep: 69 | reward: 10.641576295920794\n",
            "timestep: 70 | reward: 10.015988191817423\n",
            "timestep: 71 | reward: 3.344441480594363\n",
            "timestep: 72 | reward: 2.2979354282360527\n",
            "timestep: 73 | reward: -10\n",
            "Total T: 73 Episode Num: 3 Episode T: 32 Reward: 243.915457\n",
            "timestep: 74 | reward: 10.140242603774416\n",
            "timestep: 75 | reward: 3.071619049419638\n",
            "timestep: 76 | reward: 3.250928476475313\n",
            "timestep: 77 | reward: 3.176935314234095\n",
            "timestep: 78 | reward: 3.249332095088294\n",
            "timestep: 79 | reward: 3.6809851683452672\n",
            "timestep: 80 | reward: 3.8622261436977463\n",
            "timestep: 81 | reward: 10.076157554371939\n",
            "timestep: 82 | reward: 3.166698375139031\n",
            "timestep: 83 | reward: 10.141339982307429\n",
            "timestep: 84 | reward: 3.71514698606353\n",
            "timestep: 85 | reward: 3.8821926924842227\n",
            "timestep: 86 | reward: 3.909039100136638\n",
            "timestep: 87 | reward: 10.309222754828403\n",
            "timestep: 88 | reward: 10.402656224103653\n",
            "timestep: 89 | reward: 3.522362519567179\n",
            "timestep: 90 | reward: 10.593343409152663\n",
            "timestep: 91 | reward: 3.9231476068808075\n",
            "timestep: 92 | reward: 3.6268628814787265\n",
            "timestep: 93 | reward: 10.417857871183351\n",
            "timestep: 94 | reward: 10.004438521806158\n",
            "timestep: 95 | reward: 3.4860788205205293\n",
            "timestep: 96 | reward: 10.723602102095573\n",
            "timestep: 97 | reward: 10.776537313682294\n",
            "timestep: 98 | reward: 10.069275345144254\n",
            "timestep: 99 | reward: 10.864863784300644\n",
            "timestep: 100 | reward: 3.794047561466095\n",
            "timestep: 101 | reward: 3.911975701240232\n",
            "timestep: 102 | reward: 10.28488752226331\n",
            "timestep: 103 | reward: 10.503356600002737\n",
            "timestep: 104 | reward: 3.5882704541609693\n",
            "timestep: 105 | reward: 3.5486275880428795\n",
            "timestep: 106 | reward: 10.342145694075622\n",
            "timestep: 107 | reward: 11.089885124115135\n",
            "timestep: 108 | reward: 10.377307405654925\n",
            "timestep: 109 | reward: 10.175083225573726\n",
            "timestep: 110 | reward: 11.109132519736162\n",
            "timestep: 111 | reward: 10.145843588462354\n",
            "timestep: 112 | reward: 10.60862482946289\n",
            "timestep: 113 | reward: 11.09004206554055\n",
            "timestep: 114 | reward: 11.04501354380514\n",
            "timestep: 115 | reward: 10.226544317181746\n",
            "timestep: 116 | reward: 10.390117364135762\n",
            "timestep: 117 | reward: 10.188772393776294\n",
            "timestep: 118 | reward: 10.786654937350386\n",
            "timestep: 119 | reward: 10.472981106118437\n",
            "timestep: 120 | reward: 10.141731122884046\n",
            "timestep: 121 | reward: 10.761081846305377\n",
            "timestep: 122 | reward: 10.482588013148742\n",
            "timestep: 123 | reward: 10.571644682431799\n",
            "timestep: 124 | reward: 10.775499101352224\n",
            "timestep: 125 | reward: 10.474560206825942\n",
            "timestep: 126 | reward: 10.3920707095681\n",
            "timestep: 127 | reward: 10.098342017437862\n",
            "timestep: 128 | reward: 3.6497422199157823\n",
            "timestep: 129 | reward: 10.248034252648766\n",
            "timestep: 130 | reward: 10.0077412543681\n",
            "timestep: 131 | reward: 10.84847901302902\n",
            "timestep: 132 | reward: 11.0073930525965\n",
            "timestep: 133 | reward: 3.8738876252304624\n",
            "timestep: 134 | reward: 10.854424195845963\n",
            "timestep: 135 | reward: 10.07327442681111\n",
            "timestep: 136 | reward: 10.181828948628876\n",
            "timestep: 137 | reward: 10.023320937983959\n",
            "timestep: 138 | reward: 3.9835802481059344\n",
            "timestep: 139 | reward: 10.853285109579119\n",
            "timestep: 140 | reward: 10.477703355240637\n",
            "timestep: 141 | reward: 10.184167354558658\n",
            "timestep: 142 | reward: 10.589944167857631\n",
            "timestep: 143 | reward: 10.74282009342089\n",
            "timestep: 144 | reward: 10.186630940538096\n",
            "timestep: 145 | reward: 3.8566875410891344\n",
            "timestep: 146 | reward: 3.727443541677592\n",
            "timestep: 147 | reward: 3.8231757614249795\n",
            "timestep: 148 | reward: 3.1665185289960407\n",
            "timestep: 149 | reward: 3.1344014795112725\n",
            "timestep: 150 | reward: 3.1842338506886017\n",
            "timestep: 151 | reward: 2.96459047916313\n",
            "timestep: 152 | reward: 2.107340709228419\n",
            "timestep: 153 | reward: 2.698548265132775\n",
            "timestep: 154 | reward: 2.358994119001374\n",
            "timestep: 155 | reward: 2.7977472984780043\n",
            "timestep: 156 | reward: 2.7862592147071616\n",
            "timestep: 157 | reward: 3.8986132764302974\n",
            "timestep: 158 | reward: 3.364513954068606\n",
            "timestep: 159 | reward: 3.922713459028798\n",
            "timestep: 160 | reward: 3.1900975327095025\n",
            "timestep: 161 | reward: 3.501910464353348\n",
            "timestep: 162 | reward: 3.294829256190221\n",
            "timestep: 163 | reward: 10.405155893558614\n",
            "timestep: 164 | reward: 10.262475280679492\n",
            "timestep: 165 | reward: 3.880281405754275\n",
            "timestep: 166 | reward: 3.7914802376795333\n",
            "timestep: 167 | reward: 3.460686813848448\n",
            "timestep: 168 | reward: 3.6356602504347597\n",
            "timestep: 169 | reward: 3.4887961183335965\n",
            "timestep: 170 | reward: 2.595701232691841\n",
            "timestep: 171 | reward: 3.8480417817248735\n",
            "timestep: 172 | reward: 10.748078634110799\n",
            "timestep: 173 | reward: 3.905481459831672\n",
            "timestep: 174 | reward: 3.4090455823590595\n",
            "timestep: 175 | reward: 3.5415353704453807\n",
            "timestep: 176 | reward: 3.444470381157362\n",
            "timestep: 177 | reward: 3.490432937955498\n",
            "timestep: 178 | reward: 3.687522128165399\n",
            "timestep: 179 | reward: 10.43047811118292\n",
            "timestep: 180 | reward: 10.083067185560878\n",
            "timestep: 181 | reward: 3.642564639304355\n",
            "timestep: 182 | reward: 3.6688950237515683\n",
            "timestep: 183 | reward: 2.7056435392706373\n",
            "timestep: 184 | reward: 2.9993619262798843\n",
            "timestep: 185 | reward: 2.8456772889430155\n",
            "timestep: 186 | reward: 10.106877528495977\n",
            "timestep: 187 | reward: 3.3082616553127915\n",
            "timestep: 188 | reward: 10.648716832840524\n",
            "timestep: 189 | reward: 3.9348051906651196\n",
            "timestep: 190 | reward: 10.36765029575617\n",
            "timestep: 191 | reward: 3.8780700913161046\n",
            "timestep: 192 | reward: 3.6744036893797882\n",
            "timestep: 193 | reward: 3.7338635992865763\n",
            "timestep: 194 | reward: 10.756436107959516\n",
            "timestep: 195 | reward: 3.8314727257049164\n",
            "timestep: 196 | reward: 10.44794395008053\n",
            "timestep: 197 | reward: 3.629168101185935\n",
            "timestep: 198 | reward: 3.299150828585387\n",
            "timestep: 199 | reward: 2.460008773286075\n",
            "timestep: 200 | reward: 3.536608016778392\n",
            "timestep: 201 | reward: 2.815272693301408\n",
            "timestep: 202 | reward: 2.137743311179389\n",
            "timestep: 203 | reward: 3.815236367681886\n",
            "timestep: 204 | reward: 3.5587169330936\n",
            "timestep: 205 | reward: 3.222117969396724\n",
            "timestep: 206 | reward: 2.601955319831549\n",
            "timestep: 207 | reward: 2.5440351285906173\n",
            "timestep: 208 | reward: 2.434413703169466\n",
            "timestep: 209 | reward: 3.8879181364419004\n",
            "timestep: 210 | reward: 3.2077958534511297\n",
            "timestep: 211 | reward: 10.064024442765474\n",
            "timestep: 212 | reward: 3.858009716788405\n",
            "timestep: 213 | reward: 3.369189426681534\n",
            "timestep: 214 | reward: 2.9155386250073247\n",
            "timestep: 215 | reward: 10.396668473869388\n",
            "timestep: 216 | reward: 3.543126502758999\n",
            "timestep: 217 | reward: 3.7840869329954216\n",
            "timestep: 218 | reward: 10.035427908409355\n",
            "timestep: 219 | reward: 3.126147314108424\n",
            "timestep: 220 | reward: 3.698850915757524\n",
            "timestep: 221 | reward: 3.211922542278461\n",
            "timestep: 222 | reward: 3.7271843566869376\n",
            "timestep: 223 | reward: -10\n",
            "Total T: 223 Episode Num: 4 Episode T: 150 Reward: 940.524153\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}